# -*- coding: utf-8 -*-
"""RNN Imdb (worof ahmed).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCewyt_L9VE11U2QcOrQLat6T8Ve1IoU
"""

from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU, Bidirectional, Dropout
from tensorflow.keras.datasets import imdb
from tensorflow.keras import Sequential
import numpy as np

vocab_size = 5000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
print(x_train[0])
print(y_train[0])

word_idx = imdb.get_word_index()
idx_word = {v: k for k, v in word_idx.items()}
print(word_idx[v] for k in x_train[0])

print("Max Length of review: ", len(max((x_train + x_test), key=len)))
print("Min Length of review: ", len(min((x_train + x_test), key=len)))

from tensorflow.keras.preprocessing import sequence
max_words = 400
x_train = sequence.pad_sequences(x_train, maxlen=max_words)
x_test = sequence.pad_sequences(x_test, maxlen=max_words)
x_valid, y_valid = x_train[:64], y_train[:64]
x_train_, y_train_ = x_train[64:], y_train[64:]

x_train_, x_train

x_train_.shape

embd_len = 32
RNN_model = Sequential(name = "Simple_RNN")
RNN_model.add(Embedding(vocab_size,
embd_len,
                        input_shape=(max_words,)))
RNN_model.add(SimpleRNN(128,
                        activation='tanh',
                        return_sequences=False,
))

RNN_model.add(Dense(1, activation='sigmoid'))
RNN_model.summary()

RNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = RNN_model.fit(x_train_, y_train_, epochs=10, validation_data=(x_valid, y_valid))

print()
print(RNN_model.evaluate(x_test, y_test))

lstm_model = Sequential(name = "LSTM")
lstm_model.add(Embedding(vocab_size,
embd_len,
                        input_shape=(max_words,)))
lstm_model.add(LSTM(128,
                        activation='tanh',
                        return_sequences=False,
))
lstm_model.add(Dropout(0.5))

lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.summary()

lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history2 = lstm_model.fit(x_train_, y_train_, epochs=10, validation_data=(x_valid, y_valid))

gru_model = Sequential(name = "GRU")
gru_model.add(Embedding(vocab_size,
                        embd_len,
                        input_shape=(max_words,),

))
gru_model.add(GRU(128,
                  activation='tanh',
                  return_sequences=False))
gru_model.add(Dense(1, activation='sigmoid'))
gru_model.add(Dropout(0.5))
gru_model.summary()

gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history3 = gru_model.fit(x_train_, y_train_, batch_size=64, epochs=10, validation_data=(x_valid, y_valid))

bi_lstm_model = Sequential(name="Bidirectional")
bi_lstm_model.add(Embedding(vocab_size,
                        embd_len,
                        input_shape=(max_words,),

))
bi_lstm_model.add(Bidirectional(LSTM(128,
                  activation='tanh',
                  return_sequences=False)))
bi_lstm_model.add(Dropout(0.5))
bi_lstm_model.add(Dense(1, activation='sigmoid'))
bi_lstm_model.summary()

bi_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history4 = bi_lstm_model.fit(x_train_, y_train_, batch_size=64, epochs=10, validation_data=(x_valid, y_valid))