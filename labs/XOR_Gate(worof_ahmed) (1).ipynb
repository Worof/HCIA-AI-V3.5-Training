{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T-TrXmmyhDAz"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "-pv0M1Cphcyi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR dataset\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [1], [1], [0]])"
      ],
      "metadata": {
        "id": "uBOIFF0jhidb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "i9tZqhqvhk5P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network architecture\n",
        "input_layer_neurons = inputs.shape[1]  # Number of input features (2 for XOR)\n",
        "hidden_layer_neurons = 2  # Number of neurons in the hidden layer\n",
        "output_neurons = 1  # Single output for XOR"
      ],
      "metadata": {
        "id": "mLwcOYrOhqoG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights initialization\n",
        "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))"
      ],
      "metadata": {
        "id": "8zC5OI1-hqqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias initialization\n",
        "bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "bias_output = np.random.uniform(size=(1, output_neurons))"
      ],
      "metadata": {
        "id": "m2OpYDN4hzoN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "_qBbcEZDh0r6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the neural network\n",
        "for epoch in range(10000):\n",
        "    # Forward propagation\n",
        "    hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Backpropagation\n",
        "    error = outputs - predicted_output\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Updating weights and biases\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
        "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate"
      ],
      "metadata": {
        "id": "DnkNDhrdh3vb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the neural network\n",
        "print(\"Final predicted output after training:\")\n",
        "print(predicted_output.round())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEaBB5KCh-cO",
        "outputId": "02601f68-8b5f-486d-9b4c-724b1a0a7a8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final predicted output after training:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinal weights from input to hidden layer:\")\n",
        "print(weights_input_hidden)\n",
        "\n",
        "print(\"\\nFinal weights from hidden layer to output layer:\")\n",
        "print(weights_hidden_output)\n",
        "\n",
        "print(\"\\nFinal bias for hidden layer:\")\n",
        "print(bias_hidden)\n",
        "\n",
        "print(\"\\nFinal bias for output layer:\")\n",
        "print(bias_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMMxmvAxiYgd",
        "outputId": "30a5f824-ebcb-4600-f2d9-239eafc10314"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final weights from input to hidden layer:\n",
            "[[3.6991303  5.699086  ]\n",
            " [3.70579679 5.73069504]]\n",
            "\n",
            "Final weights from hidden layer to output layer:\n",
            "[[-8.02663537]\n",
            " [ 7.42116082]]\n",
            "\n",
            "Final bias for hidden layer:\n",
            "[[-5.67051588 -2.37581163]]\n",
            "\n",
            "Final bias for output layer:\n",
            "[[-3.35000977]]\n"
          ]
        }
      ]
    }
  ]
}